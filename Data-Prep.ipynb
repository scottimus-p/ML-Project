{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9347951a-1392-4c3c-a08c-badca0c5781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfda6510-282a-4ae6-8a1c-63c8f30cfc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af21bf44-4123-4e6a-9abc-8e22f3fcb6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScenarioDataset(Dataset):\n",
    "    def __init__(self, scenarios):\n",
    "        super(ScenarioDataset, self).__init__()\n",
    "        self.scenarios = scenarios\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scenarios)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.scenarios[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6429b265-7f78-4728-9bd3-da0915093c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_path = \"D:\\\\Scenario-Generator\\\\\"\n",
    "output_path = \"D:\\\\GMIB\\\\\"\n",
    "\n",
    "markets = ['USDiversified', 'International', 'Intermediate', 'Aggressive', 'MoneyMkt', 'MedGovt', 'LongCorp']\n",
    "timesteps = 1200\n",
    "\n",
    "def read_scenario(scn_path, scenario_num):\n",
    "    scn_path = f'{scn_path}\\scenario_{scenario_num}.json'\n",
    "    with open(scn_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    market_data = []\n",
    "    for m in markets:\n",
    "        market_data.append(np.array(data['equities'][m][0:timesteps]) * 100)\n",
    "    \n",
    "    reformatted_data = list(zip(*market_data))\n",
    "    return torch.as_tensor(reformatted_data, device=device)\n",
    "    \n",
    "    \n",
    "scenario_lower_bound = 1\n",
    "scenario_upper_bound = scenario_lower_bound + 10000\n",
    "data_all_scenarios = []\n",
    "for i in range(scenario_lower_bound, scenario_upper_bound):\n",
    "    tensor_this_scenario = read_scenario(scenario_path, i)\n",
    "    data_all_scenarios.append(tensor_this_scenario)\n",
    "\n",
    "dataset = ScenarioDataset(data_all_scenarios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5f8cde1-fcaa-4ca2-bf9e-c4cfee39f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Source: An assignment from my NLP class (CS388)\n",
    "#\n",
    "# Implementation of positional encoding that you can use in your network\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, num_positions: int=20, batched=False, add=False):\n",
    "        \"\"\"\n",
    "        :param d_model: dimensionality of the embedding layer to your model; since the position encodings are being\n",
    "        added to character encodings, these need to match (and will match the dimension of the subsequent Transformer\n",
    "        layer inputs/outputs)\n",
    "        :param num_positions: the number of positions that need to be encoded; the maximum sequence length this\n",
    "        module will see\n",
    "        :param batched: True if you are using batching, False otherwise\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Dict size\n",
    "        self.emb = torch.nn.Embedding(num_positions, d_model)\n",
    "        self.batched = batched\n",
    "        self.add = add\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: If using batching, should be [batch size, seq len, embedding dim]. Otherwise, [seq len, embedding dim]\n",
    "        :return: a tensor of the same size with positional embeddings added in\n",
    "        \"\"\"\n",
    "        # Second-to-last dimension will always be sequence length\n",
    "        input_size = x.shape[-2]\n",
    "        indices_to_embed = torch.tensor(np.asarray(range(0, input_size))).type(torch.LongTensor).to(device)\n",
    "        if self.batched:\n",
    "            # Use unsqueeze to form a [1, seq len, embedding dim] tensor -- broadcasting will ensure that this\n",
    "            # gets added correctly across the batch\n",
    "            emb_unsq = self.emb(indices_to_embed).unsqueeze(0)\n",
    "            if self.add:\n",
    "                return x + emb_unsq\n",
    "            else:\n",
    "                return x - emb_unsq\n",
    "        else:\n",
    "            if self.add:\n",
    "                return x + self.emb(indices_to_embed)\n",
    "            else:\n",
    "                return x - self.emb(indices_to_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1861c34b-4bad-4413-beb7-97cc77062736",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, input_shape, batched=False):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_shape[1]\n",
    "        self.sequence_length = input_shape[0]\n",
    "        self.add_positional_encoding = PositionalEncoding(self.hidden_dim, timesteps, batched=True, add=True)\n",
    "        self.sub_positional_encoding = PositionalEncoding(self.hidden_dim, timesteps, batched=True, add=False)\n",
    "        self.embed_market_data = torch.nn.Linear(in_features=self.input_dim, out_features=self.hidden_dim)\n",
    "        self.encoder_layer = torch.nn.TransformerEncoderLayer(d_model=self.hidden_dim,\n",
    "                                                              nhead=2,\n",
    "                                                              dim_feedforward=512,\n",
    "                                                              dropout=0.1,\n",
    "                                                              activation='relu',\n",
    "                                                              batch_first=True)\n",
    "        self.encoder = torch.nn.TransformerEncoder(self.encoder_layer, num_layers=1, norm=None, enable_nested_tensor=True, mask_check=False)\n",
    "        self.downscale = torch.nn.Linear(in_features=self.hidden_dim * timesteps, out_features=64)\n",
    "        self.upscale = torch.nn.Linear(in_features=64, out_features=self.hidden_dim * timesteps)\n",
    "        self.decode_layer = torch.nn.TransformerEncoderLayer(d_model=self.hidden_dim,\n",
    "                                                              nhead=2,\n",
    "                                                              dim_feedforward=512,\n",
    "                                                              dropout=0.1,\n",
    "                                                              activation='relu',\n",
    "                                                              batch_first=True)\n",
    "\n",
    "        self.decoder = torch.nn.TransformerEncoder(self.decode_layer, num_layers=1, norm=None, enable_nested_tensor=True, mask_check=False)        \n",
    "        self.unembed_market_data = torch.nn.Linear(in_features=self.hidden_dim, out_features=self.input_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "        #encoding\n",
    "        src = self.embed_market_data(src)\n",
    "        src = self.add_positional_encoding(src)\n",
    "        enc = self.encoder(src)\n",
    "        enc = self.downscale(enc.flatten(start_dim=1))\n",
    "        \n",
    "        #decoding\n",
    "        dec = self.upscale(enc).unflatten(1, (self.sequence_length, self.hidden_dim))\n",
    "\n",
    "        dec = self.decoder(dec)\n",
    "        dec = self.sub_positional_encoding(src)\n",
    "        dec = self.unembed_market_data(dec)\n",
    "        \n",
    "        return dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c092392a-51e6-4a7a-a5b2-e7540d333326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1445.0559868205644\n",
      "epoch: 1, loss: 34.311859591153166\n",
      "epoch: 2, loss: 12.503643146053482\n",
      "epoch: 3, loss: 1.7070446325953816\n",
      "epoch: 4, loss: 1.1706659545435445\n",
      "epoch: 5, loss: 1.1601854119336914\n",
      "epoch: 6, loss: 1.1589834572214661\n",
      "epoch: 7, loss: 1.1579294626084025\n",
      "epoch: 8, loss: 1.1564447811742082\n",
      "epoch: 9, loss: 1.1524882183068093\n",
      "Duration: 908.1584424972534\n"
     ]
    }
   ],
   "source": [
    "# Create the network\n",
    "model = Model(hidden_dim=6, input_shape=(timesteps, len(markets)), batched=True)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create the loss\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "\n",
    "        l = loss(output, batch)\n",
    "        \n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute the accuracy\n",
    "        total_loss += l.cpu().detach()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch: {epoch}, loss: {total_loss}')\n",
    "    if total_loss < 1e-2:\n",
    "        break\n",
    "    \n",
    "print('Duration: {}'.format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56ad76f1-b759-4821-acba-e6ca52f59fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1679e+00, -6.4703e+00, -2.7507e+00,  ..., -3.0700e-02,\n",
       "          9.0000e-04,  1.1400e-02],\n",
       "        [ 3.6363e+00,  3.5294e+00,  3.9141e+00,  ..., -3.4900e-02,\n",
       "          6.6500e-02,  4.7000e-03],\n",
       "        [ 4.1800e-02,  2.7178e+00, -1.0438e+01,  ..., -3.8900e-02,\n",
       "         -2.8500e-02, -4.0960e-01],\n",
       "        ...,\n",
       "        [-1.6466e+00, -3.6367e+00, -4.5058e+00,  ...,  2.7060e-01,\n",
       "          4.0240e-01, -3.4500e-02],\n",
       "        [ 8.0416e+00,  7.9011e+00,  9.3764e+00,  ...,  1.3630e-01,\n",
       "          1.9250e+00,  3.0164e+00],\n",
       "        [-7.2116e+00, -8.0554e+00, -6.0846e+00,  ...,  2.9300e-02,\n",
       "          1.3744e+01,  2.3511e+01]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "521db6c0-cb4b-4e8f-9f09-569e3fe9bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(dataset[0][None,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f035bc4e-86e5-455d-bc69-a3c4a9d7691c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1678e+00, -6.4702e+00, -2.7503e+00,  ...,  1.4913e-01,\n",
      "          -1.7830e-03,  1.2697e-02],\n",
      "         [ 3.6365e+00,  3.5298e+00,  3.9147e+00,  ...,  1.2787e-01,\n",
      "           6.4083e-02,  6.2005e-03],\n",
      "         [ 4.1994e-02,  2.7181e+00, -1.0437e+01,  ...,  1.1622e-01,\n",
      "          -3.0672e-02, -4.0854e-01],\n",
      "         ...,\n",
      "         [-1.6466e+00, -3.6367e+00, -4.5061e+00,  ...,  1.5597e-01,\n",
      "           4.0389e-01, -3.5433e-02],\n",
      "         [ 8.0416e+00,  7.9011e+00,  9.3764e+00,  ...,  1.3281e-01,\n",
      "           1.9253e+00,  3.0162e+00],\n",
      "         [-7.2115e+00, -8.0552e+00, -6.0837e+00,  ...,  1.8161e-01,\n",
      "           1.3743e+01,  2.3512e+01]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c87d680-ff19-405f-a211-739ca047d7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
